<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Visual Reasoning and Retroduction</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
  <style>
    body {
      margin: 0;
      font-family: 'Inter', sans-serif;
      background-color: #fff;
      color: #111;
      text-align: center;
      padding: 50px 20px;
      font-size: 18px;
      line-height: 1.6;
    }

    h1 {
      font-size: 2.5rem;
      font-weight: 700;
      margin: 0.6em 0 0.3em;
    }

    h2 {
      font-style: italic;
      font-weight: 400;
      color: #333;
      margin: 0.2em 0 1em;
      font-size: 1.5rem;
    }

    .authors {
      font-size: 1.1rem;
      color: #1d4ed8;
      margin-bottom: 0.6em;
    }

    .affiliations {
      font-size: 1rem;
      color: #555;
      margin-bottom: 1.2em;
    }

    .conference {
      font-size: 1.3rem;
      font-weight: bold;
      margin-bottom: 1.8em;
    }

    .buttons a {
      display: inline-block;
      padding: 12px 24px;
      margin: 0 10px;
      background-color: #1f2937;
      color: white;
      border-radius: 9999px;
      font-weight: 600;
      text-decoration: none;
      font-size: 1rem;
      transition: background-color 0.3s;
    }

    .buttons a:hover {
      background-color: #4b5563;
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      text-align: left;
    }

    .preview-image {
      margin: 40px 0 30px;
      text-align: center;
    }

    .preview-image img {
      width: 100%;
      height: auto;
      border-radius: 10px;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
    }

    .section h3 {
      font-size: 1.7rem;
      margin-bottom: 0.6em;
    }

    .section p {
      font-size: 1.1rem;
      line-height: 1.7;
      color: #333;
    }

    footer {
      margin-top: 60px;
      font-size: 0.95rem;
      color: #888;
    }
  </style>
</head>
<body>

  <h1>Visual Reasoning and Retroduction</h1>
  <h2>Deep Understanding in Cartoon Scenarios: Redefining Inference Paradigm</h2>

  <div class="authors">
    Yubing Li<sup>1</sup>, Xuesong Tang<sup>1*</sup>, Mingbo Zhao<sup>1</sup>
  </div>
  <div class="affiliations">
    <sup>1</sup>Donghua University &nbsp;&nbsp;
    *Corresponding author
  </div>

  <div class="conference"></div>

  <div class="buttons">
    <a href="#" target="_blank">ðŸ“„ Paper</a>
    <a href="https://github.com/CaesarrrrLyb/VfRrR" target="_blank">ðŸ’» Code</a>
    <a href="#" target="_blank">ðŸ“· Example</a>
  </div>

  <div class="container">
    <div class="preview-image">
      <img src="intro_fig/intro_fig.png" alt="Project Preview">
    </div>

    <div class="section">
      <h3>Abstract</h3>
      <p>
        Multimodal Large Language Models (MLLMs) have demonstrated promising capabilities in visual reasoning tasks. However, their capabilities of intelligence and comprehension are more plausibly driven by the exploitation of statistical correlations rather than by genuine deep understanding. To address this limitation and investigate the intrinsic reasoning capabilities of AI models, we propose a dedicated task: Visual Reasoning and Retroduction (VRR). This task unifies forward reasoning and backward retroduction into a novel inference paradigm that requires deducing the most plausible future or past event. To support this task, we construct EPIC (Peppa Logic), a dataset specifically designed for logical inference. It includes scenario sequences, corresponding text descriptions, and explicit cause and aftermath annotations of each scenario. In addition, inspired by human cognitive behavior, a bio-inspired model called ScenarioInfer was developed motivated by the neural responses of the parahippocampal place area and retrosplenial complex during reasoning. Experimental results on both open-world and enclosed-world datasets demonstrate that our proposed model achieves state-of-the-art performance on VRR, outperforming the latest MLLMs. This new paradigm not only advances the field of visual reasoning but also provides a robust foundation for exploring logical reasoning in AI systems.
      </p>
    </div>
  </div>

  <footer>
    &copy; 2025 Yubing Li. All rights reserved.
  </footer>

</body>
</html>
